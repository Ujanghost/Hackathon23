# -*- coding: utf-8 -*-
"""Jugaad Q-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuzSHpbEiOhY-c2txNFKkTppHt3AIQfa
"""

import pandas as pd
import streamlit as st



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline




data=pd.read_csv("encoded.csv")

data

correlation_matrix = data.corr()



# Display correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix[['G3']], annot=True, cmap='coolwarm')
plt.title('Correlation Matrix for Final Grade (G3)')
plt.show()

# # Identify categorical and numerical columns
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

# # Preprocessing for numerical data
numerical_transformer = Pipeline(steps=[
     ('imputer', SimpleImputer(strategy='mean')),
     ('scaler', StandardScaler())])

# # Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
     ('imputer', SimpleImputer(strategy='most_frequent')),
     ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# # Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
     transformers=[
         ('num', numerical_transformer, numerical_cols),
         ('cat', categorical_transformer, categorical_cols)])

# Extract relevant features
selected_features = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',
       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',
       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',
       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',
       'Walc', 'health', 'absences', 'G1', 'G2', 'G3']

extracted_features = data[selected_features]

# Remove target from predictors
X = data.drop(['G3'], axis=1)
y = data['G3']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=82)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Extract feature importance
feature_importance = model.feature_importances_

# Display feature importance scores
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plotting feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance')
plt.show()

st.write("Feature Importance:")
st.write(feature_importance_df)

#Select top features based on importance (you can adjust the number)
top_features = feature_importance_df.head(31)['Feature'].tolist()

# Prepare the data with selected features
X_train_selected = X_train[top_features]
X_test_selected = X_test[top_features]

"""Linear Regression"""

# Train a Linear Regression model with selected features
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression()
lr_model.fit(X_train_selected, y_train)

# # # Make predictions
predictions = lr_model.predict(X_test_selected)

# # Evaluate the model
mse = mean_squared_error(y_test, predictions)
st.write(f'Mean Squared Error: {mse}')

r_squared = r2_score(y_test, predictions)
st.write(r_squared)
st.write(r_squared)

"""Random Forest"""

# Train a model
model = RandomForestRegressor()
model.fit(X_train_selected, y_train)

# # Make predictions
predictions = model.predict(X_test_selected)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
st.write(f'Mean Squared Error: {mse}')

r_squared = r2_score(y_test, predictions)
st.write(r_squared)

"""XG-Boost"""

from xgboost import XGBRegressor  # Import XGBRegressor from xgboost

# Training the XGBoost Regressor
model = XGBRegressor(n_estimators=100, random_state=42)  # You can adjust the parameters as needed
model.fit(X_train_scaled, y_train)

# Make predictions
predictions = model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, predictions)
st.write(f'Mean Squared Error: {mse}')

r_squared = r2_score(y_test, predictions)
st.write(r_squared)

"""How does a studentâ€™s social life affect the quality of education?"""

